{
 "cells": [
  {
   "cell_type": "raw",
   "id": "1477f7db-9cff-436c-b516-10eb55a33e4b",
   "metadata": {},
   "source": [
    "Objective:\n",
    "\n",
    "The objective of this assignment is to apply various data preprocessing techniques on a given \n",
    "dataset to clean and prepare it for further analysis."
   ]
  },
  {
   "cell_type": "raw",
   "id": "2d45cd8c-707e-4611-a3cd-f92226eca952",
   "metadata": {},
   "source": [
    "# ===== Task 1: Imports & Load Dataset ===== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d95b7a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded file: train.csv\n",
      "Shape: (891, 12)\n",
      "\n",
      "First 5 rows:\n",
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# Try known uploaded path first, then fallback to local file name\n",
    "possible_paths = ['/mnt/data/train.csv', 'train.csv']\n",
    "for p in possible_paths:\n",
    "    if os.path.exists(p):\n",
    "        data_path = p\n",
    "        break\n",
    "else:\n",
    "    raise FileNotFoundError(\"train.csv not found in /mnt/data or current directory.\")\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "df_original = df.copy()   # keep an untouched copy\n",
    "print(\"Loaded file:\", data_path)\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bcc37198-2605-478f-aff8-f0bc14659d33",
   "metadata": {},
   "source": [
    "# ===== Task 2: Handle Missing Values ===== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "848c102d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values (count) per column:\n",
      "PassengerId    0\n",
      "Survived       0\n",
      "Pclass         0\n",
      "Name           0\n",
      "Sex            0\n",
      "Age            0\n",
      "SibSp          0\n",
      "Parch          0\n",
      "Ticket         0\n",
      "Fare           0\n",
      "Embarked       0\n",
      "dtype: int64\n",
      "\n",
      "Columns with >50% missing (will drop): []\n",
      "\n",
      "Numerical columns detected: ['PassengerId', 'Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\n",
      "\n",
      "Categorical columns detected: ['Name', 'Sex', 'Ticket', 'Embarked']\n",
      "\n",
      "Missing values after imputation:\n",
      "PassengerId    0\n",
      "Survived       0\n",
      "Pclass         0\n",
      "Name           0\n",
      "Sex            0\n",
      "Age            0\n",
      "SibSp          0\n",
      "Parch          0\n",
      "Ticket         0\n",
      "Fare           0\n",
      "Embarked       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1) Show missing values per column\n",
    "print(\"Missing values (count) per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# 2) Drop columns with >50% missing values\n",
    "missing_frac = df.isnull().mean()\n",
    "cols_to_drop = missing_frac[missing_frac > 0.5].index.tolist()\n",
    "print(\"\\nColumns with >50% missing (will drop):\", cols_to_drop)\n",
    "df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "# 3) Fill missing numerical values with median\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(\"\\nNumerical columns detected:\", num_cols)\n",
    "for col in num_cols:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        med = df[col].median()\n",
    "        df[col].fillna(med, inplace=True)\n",
    "        print(f\"Filled missing numeric column '{col}' with median = {med}\")\n",
    "\n",
    "# 4) Fill missing categorical values with mode\n",
    "cat_cols = df.select_dtypes(include=['object','category']).columns.tolist()\n",
    "print(\"\\nCategorical columns detected:\", cat_cols)\n",
    "for col in cat_cols:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        mode_val = df[col].mode()[0]\n",
    "        df[col].fillna(mode_val, inplace=True)\n",
    "        print(f\"Filled missing categorical column '{col}' with mode = '{mode_val}'\")\n",
    "\n",
    "print(\"\\nMissing values after imputation:\")\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2bc196c2-e55c-48a1-88ed-74b78cf3ff76",
   "metadata": {},
   "source": [
    "# ===== Task 3: Handle Duplicate Data ===== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9879a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_before = df.duplicated().sum()\n",
    "print(\"Duplicate rows found before removal:\", dup_before)\n",
    "\n",
    "if dup_before > 0:\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    print(\"Duplicates removed.\")\n",
    "else:\n",
    "    print(\"No duplicate rows found.\")\n",
    "\n",
    "dup_after = df.duplicated().sum()\n",
    "print(\"Duplicate rows after removal:\", dup_after)\n",
    "print(\"Shape after duplicate removal:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a21241fe-ba6f-43f9-94e3-45c108f78773",
   "metadata": {},
   "source": [
    "# ===== Task 4: Convert Categorical Features to Numeric ===== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd069498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Column 'Name' is text-heavy. Consider dropping it for modeling.\n",
      "NOTE: Column 'Ticket' is text-heavy. Consider dropping it for modeling.\n",
      "Columns to one-hot encode: ['Name', 'Sex', 'Ticket', 'Embarked']\n",
      "Shape after one-hot encoding: (891, 1584)\n",
      "\n",
      "Columns sample after encoding: ['PassengerId', 'Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Name_Abbing, Mr. Anthony', 'Name_Abbott, Mr. Rossmore Edward', 'Name_Abbott, Mrs. Stanton (Rosa Hunt)', 'Name_Abelson, Mr. Samuel', 'Name_Abelson, Mrs. Samuel (Hannah Wizosky)', 'Name_Adahl, Mr. Mauritz Nils Martin', 'Name_Adams, Mr. John', 'Name_Ahlin, Mrs. Johan (Johanna Persdotter Larsson)', 'Name_Aks, Mrs. Sam (Leah Rosen)', 'Name_Albimona, Mr. Nassef Cassem', 'Name_Alexander, Mr. William', 'Name_Alhomaki, Mr. Ilmari Rudolf', 'Name_Ali, Mr. Ahmed', 'Name_Ali, Mr. William', 'Name_Allen, Miss. Elisabeth Walton', 'Name_Allen, Mr. William Henry', 'Name_Allison, Master. Hudson Trevor', 'Name_Allison, Miss. Helen Loraine', 'Name_Allison, Mrs. Hudson J C (Bessie Waldo Daniels)', 'Name_Allum, Mr. Owen George', 'Name_Andersen-Jensen, Miss. Carla Christine Nielsine', 'Name_Anderson, Mr. Harry', 'Name_Andersson, Master. Sigvard Harald Elias', 'Name_Andersson, Miss. Ebba Iris Alfrida', 'Name_Andersson, Miss. Ellis Anna Maria', 'Name_Andersson, Miss. Erna Alexandra', 'Name_Andersson, Miss. Ingeborg Constanzia', 'Name_Andersson, Miss. Sigrid Elisabeth', 'Name_Andersson, Mr. Anders Johan', 'Name_Andersson, Mr. August Edvard (\"Wennerstrom\")', 'Name_Andersson, Mrs. Anders Johan (Alfrida Konstantia Brogren)', 'Name_Andreasson, Mr. Paul Edvin', 'Name_Andrew, Mr. Edgardo Samuel']\n"
     ]
    }
   ],
   "source": [
    "# Optional: drop very high-cardinality textual fields (uncomment if desired)\n",
    "for col in ['Name','Ticket']:\n",
    "    if col in df.columns:\n",
    "        print(f\"NOTE: Column '{col}' is text-heavy. Consider dropping it for modeling.\")\n",
    "        # df.drop(columns=[col], inplace=True)   # <- uncomment to drop\n",
    "\n",
    "# Convert boolean dtype columns to 0/1\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'bool':\n",
    "        df[col] = df[col].astype(int)\n",
    "        print(f\"Converted boolean column '{col}' to int (0/1).\")\n",
    "\n",
    "# Identify categorical columns to one-hot encode\n",
    "cat_cols = df.select_dtypes(include=['object','category']).columns.tolist()\n",
    "print(\"Columns to one-hot encode:\", cat_cols)\n",
    "\n",
    "# Apply one-hot encoding (creates new columns and drops the originals)\n",
    "df = pd.get_dummies(df, columns=cat_cols, drop_first=False)\n",
    "print(\"Shape after one-hot encoding:\", df.shape)\n",
    "\n",
    "# If you want to see a few columns after encoding:\n",
    "print(\"\\nColumns sample after encoding:\", list(df.columns)[:40])\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ad148ba6-0926-48d1-b9a2-43b15f9c9370",
   "metadata": {},
   "source": [
    "# ===== Task 5: Feature Scaling (Min-Max and StandardScaler) ===== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023ef0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# determine numeric columns excluding target/id\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "for c in ['PassengerId','Survived']:\n",
    "    if c in numeric_cols:\n",
    "        numeric_cols.remove(c)\n",
    "print(\"Numeric columns to scale:\", numeric_cols)\n",
    "\n",
    "# 1) Min-Max scaling (0-1)\n",
    "minmax_scaler = MinMaxScaler()\n",
    "df_minmax = df.copy()\n",
    "df_minmax[numeric_cols] = minmax_scaler.fit_transform(df_minmax[numeric_cols])\n",
    "print(\"\\nMin-Max scaling applied. Summary (min, max, mean, std):\")\n",
    "print(df_minmax[numeric_cols].describe().T[['min','max','mean','std']])\n",
    "\n",
    "# 2) StandardScaler (mean=0, std=1)\n",
    "std_scaler = StandardScaler()\n",
    "df_standard = df.copy()\n",
    "df_standard[numeric_cols] = std_scaler.fit_transform(df_standard[numeric_cols])\n",
    "print(\"\\nStandard scaling applied. Summary (min, max, mean, std):\")\n",
    "print(df_standard[numeric_cols].describe().T[['min','max','mean','std']])\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "889c8851-26eb-44bc-ace2-8f378da4a822",
   "metadata": {},
   "source": [
    "# ===== Task 6: Outlier Detection using IQR Method ===== #"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aefe95ee-e770-49a3-9eaf-0805c9ce9ee6",
   "metadata": {},
   "source": [
    "# ===== Task 6: Outlier Detection using IQR Method ===== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "537f12ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'Pclass': No outliers detected\n",
      "Column 'Age': 66 outliers replaced with median = 28.0\n",
      "Column 'SibSp': 46 outliers replaced with median = 0.0\n",
      "Column 'Parch': 213 outliers replaced with median = 0.0\n",
      "Column 'Fare': 116 outliers replaced with median = 14.4542\n",
      "\n",
      "Outliers count per column (before replacement):\n",
      "{'Pclass': 0, 'Age': 66, 'SibSp': 46, 'Parch': 213, 'Fare': 116}\n"
     ]
    }
   ],
   "source": [
    "# We'll detect outliers on numeric_cols (the same list used above) and replace them with the median.\n",
    "df_outliers_handled = df.copy()   # start from encoded + imputed dataset\n",
    "\n",
    "# Ensure numeric_cols list excludes target/id if present\n",
    "numeric_cols = df_outliers_handled.select_dtypes(include=[np.number]).columns.tolist()\n",
    "for c in ['PassengerId','Survived']:\n",
    "    if c in numeric_cols:\n",
    "        numeric_cols.remove(c)\n",
    "\n",
    "outlier_summary = {}\n",
    "for col in numeric_cols:\n",
    "    Q1 = df_outliers_handled[col].quantile(0.25)\n",
    "    Q3 = df_outliers_handled[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    mask = (df_outliers_handled[col] < lower) | (df_outliers_handled[col] > upper)\n",
    "    cnt = int(mask.sum())\n",
    "    outlier_summary[col] = cnt\n",
    "    if cnt > 0:\n",
    "        med = df_outliers_handled[col].median()\n",
    "        df_outliers_handled.loc[mask, col] = med\n",
    "        print(f\"Column '{col}': {cnt} outliers replaced with median = {med}\")\n",
    "    else:\n",
    "        print(f\"Column '{col}': No outliers detected\")\n",
    "\n",
    "print(\"\\nOutliers count per column (before replacement):\")\n",
    "print(outlier_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3a69615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: cleaned_no_scaling.csv (shape: (891, 1584))\n",
      "Saved: cleaned_minmax.csv (shape: (891, 1584))\n",
      "Saved: cleaned_standard.csv (shape: (891, 1584))\n",
      "\n",
      "Done. Three cleaned files saved: cleaned_no_scaling.csv, cleaned_minmax.csv, cleaned_standard.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===== Final: Scale again (optional) and Save cleaned datasets =====\n",
    "# If you want the final saved datasets with scaling applied after outlier handling, create them:\n",
    "\n",
    "# numeric columns to scale (same logic as before)\n",
    "final_numeric_cols = df_outliers_handled.select_dtypes(include=[np.number]).columns.tolist()\n",
    "for c in ['PassengerId','Survived']:\n",
    "    if c in final_numeric_cols:\n",
    "        final_numeric_cols.remove(c)\n",
    "\n",
    "# MinMax scaled final\n",
    "final_minmax = df_outliers_handled.copy()\n",
    "final_minmax[final_numeric_cols] = MinMaxScaler().fit_transform(final_minmax[final_numeric_cols])\n",
    "\n",
    "# Standard scaled final\n",
    "final_standard = df_outliers_handled.copy()\n",
    "final_standard[final_numeric_cols] = StandardScaler().fit_transform(final_standard[final_numeric_cols])\n",
    "\n",
    "# Save CSVs (these files will be saved in the current working directory or notebook workspace)\n",
    "final_files = {\n",
    "    'cleaned_no_scaling.csv': df_outliers_handled,\n",
    "    'cleaned_minmax.csv': final_minmax,\n",
    "    'cleaned_standard.csv': final_standard\n",
    "}\n",
    "for fname, dataframe in final_files.items():\n",
    "    dataframe.to_csv(fname, index=False)\n",
    "    print(f\"Saved: {fname} (shape: {dataframe.shape})\")\n",
    "\n",
    "print(\"\\nDone. Three cleaned files saved: cleaned_no_scaling.csv, cleaned_minmax.csv, cleaned_standard.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
